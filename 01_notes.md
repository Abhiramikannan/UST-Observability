28/07/2025

Observability:
Observability is the ability to understand what is happening inside a system just by looking at its external outputs (like logs, metrics, and traces). 
It helps teams detect, diagnose, and resolve issues in complex software systems.


1. What is the cost of downtime?
	morethan 150 dollars
2. Downtime: Downtime simply means the time when a system, website, or service is not working or unavailable. eg: If a website is down for 2 hours due to maintenance, that 2 hours is called downtime.

 In short: "Time when something is not working."
 
3. reliablity and resilience?
	reliability->ability of a system to perform its desired function without undergoing failure

	resilience: fails but how to come back to norml state?

4. if code is not optimized-code optimization needed.
5. code optimization: Improve the performance of the code i.e, making it faster, use less memory, or consume fewer resources.
   
6. refactoring? companies focus?-helps to do code optimization.
7. refactoring: Improving the structure, readability, and maintainability of code without changing its functionality.
   
8. Monitoring
   
	infrastructure monitoring

	application monitoring

threshold will be defined in monitoring,if it goes above-need action

eg: are u healthy? you will be looking ur sugar level,values-> if it goes beyond threshhold->take actions.

in monitoring -> multiple reasons will be there to fail-> observability comes into picture

what percentage of issues coming again and again on ur service desk?
	automatically creating ticketing service
	organization looks->preventing incidents happening again

 KEY TERMS:
 
 1. Metrics – "Numbers that show performance over time"
    		Helps monitor system health and performance trends.
    	Example: CPU usage = 85%, Memory = 70%, Requests per second = 200.

 2. Logs – "Detailed records of events"
    		 Text records generated by systems/apps to describe what happened.
    	eg: "User login failed at 3:25 PM due to invalid password."
    Helps diagnose problems and understand what went wrong.
    
 3. Traces – "The path of a request through the system"
    		A trace shows how a request flows through different services.
    	eg: A user places an order → goes through payment → shipping → success.
    	Use: Helps detect where delays or errors happened in complex systems.

     
 4. Dashboards – "Visual panels to monitor everything in one place"
    		 A visual interface that shows metrics, logs, and traces in graphs, charts, etc.
    		 Gives real-time insights and alerts to monitor system health quickly.

BIG PUZZLE:
<img width="1354" height="845" alt="image" src="https://github.com/user-attachments/assets/5cab3c59-9b08-4236-8563-12756fc375b2" />

 1. Known-Knowns :
    We know the problem AND the solution.
    have 10 no of users,move to 100 no of users->scale(could be on cloud,on-premise ->depends on saying that u defined the threshholds)
    eg: You know CPU usage goes high when user traffic increases.
        You already know to scale the app up in such cases.
        Tools/Actions:Dashboards,Dynamic thresholding,Alerts,Automation
        Goal: Make what you know work even better (optimize).
 2. Known-Unknowns:
    We know something is wrong, but we don’t know why.
    eg: Latency spike is visible in the dashboard, but you don’t know the root cause.
         Latency is the time between a user making a request and the system responding to it.
         A latency spike means the response time suddenly increased — maybe for a few seconds, or minutes.
         Your normal response time = 200ms
         But suddenly, it becomes = 1.5 seconds
         tools/actions: Auto-correlation,Real-time analytics,Business process mining
    Goal: Use smart tools to dig into the data and figure out the reason.
 3. Unknown-Knowns:
    We have the data but didn’t realize something’s wrong until analysis.
         eg: You already collect disk usage metrics, but never noticed it's been slowly increasing.Once you do anomaly detection, you discover this issue.
         Tools/Actions: Anomaly detection,Predictive alerts,Log analysis
    Goal: Turn hidden problems into known ones before they cause failure.
    anolomy detection: where u look upon ur existing system.Because u defined ur monitoring,threshhold->see whats deviating: maybe ur cpu,memory,etc.

4. Unknown-Unknowns:
   We don’t know there’s a problem, and we don’t know what to look for.
   eg:
        Suddenly your system fails for no apparent reason,
        There were no symptoms. Everything looked fine,
        Like a sudden heart attack with no warning.
    Tools/Actions: Deep observability, AI/ML to detect patterns, Flexible monitoring (ingest all data, open-ended queries)
    Goal: Prepare for the unexpected. Build a system that can tell you: "Hey, something strange is happening, even if you didn’t know to check for it!"

key points:
We want to become predictable — to avoid surprises.We can’t know everything in advance — especially unknown unknowns.That’s where observability becomes powerful:It helps us see what we didn’t even think to monitor.

    

CORE ANALYSIS LOOP:
<img width="1391" height="707" alt="image" src="https://github.com/user-attachments/assets/7c4258f1-87ec-45fb-927f-07254435a38d" />

telemetry data: logs,metrices,traces

steps:
1. What are you trying to understand?
     Example: “Why did our application latency spike yesterday?” or “Why is there a sudden drop in CPU usage?”
2. Visualize telemetry data to find relevant performance anomalies
    Look at metrics, logs, traces, dashboards, etc., to spot irregularities.
    Tools: Azure Monitor, Grafana, Prometheus, etc.
    Goal: Identify where and when the problem starts.
3. Search for common dimensions within the anomalous area
     Group by service name?
     Filter by region?
     version (v1.2.3 vs v1.2.4)
     user type
     instance(pod1,pod2)
     Compare time ranges?
   This helps narrow down the issue to patterns or causes.
   You group or filter by these dimensions to find a pattern.
   “Is this issue happening only for certain pods? Or only after a new deployment? Or only in one region?”

5. Evaluate
     “Did my filtering/grouping help me find something meaningful?”
   if yes-> got a clue ->“Only version v1.2.4 of the app is failing in us-east region.”->Go back to step 1 with a narrower question:What changed in version v1.2.4?->You now dig deeper into the real root cause.

   if no->You didn’t find anything useful yet.->So, go back to Step 2 or 3, and try filtering using different attributes.->This time filter by customer type,check API endpoint paths.

Why This Is a Loop?
  Visualize → Filter → Think → Repeat

| Step           | What you do                                                 | Example                                            |
| -------------- | ----------------------------------------------------------- | -------------------------------------------------- |
| 1. Define Goal | Ask what you want to understand                             | Why is login slow?                                 |
| 2. Visualize   | Check dashboards/logs/metrics                               | Latency spike at 3 PM                              |
| 3. Analyze     | Filter/group by attributes                                  | Only `v1.2.4` in `us-east` is affected             |
| 4. Evaluate    | Did this help? If yes, go deeper. If not, try another view. | Found faulty pod, or repeat with different filters |

SYMPTOMS VS CAUSES:
<img width="1423" height="852" alt="image" src="https://github.com/user-attachments/assets/ab37c224-7aa8-4dc5-bc25-c1d4d1a0200a" />
 1. Symptoms (Top of the Temple):
      “Something is broken” – you can see the impact, but not yet know why.
       Examples:High latency, Errors in dashboard, App not loading, Service down
    This is what traditional monitoring shows — the “what”.
    Monitoring: when it reaches above threshold(cpu,memory) -we take actions(alerts).

    
 2. The Pillars (Middle) — Logs, Metrics, Traces, etc.
      These are your data signals — used to investigate and debug. They help answer the question:"Why is this broken?"
    
| Pillar       | What it helps you do                                           |
| ------------ | -------------------------------------------------------------- |
| **Logs**     | Detailed events and errors (e.g. error 500 in payment service) |
| **Events**   | Changes like deployments, restarts                             |
| **Metrics**  | Numbers over time (CPU, memory, latency, request count)        |
| **Profiles** | System behavior (CPU time per function, memory leaks)          |
| **Traces**   | End-to-end request flow (microservices call chain)             |

These are used during debugging, profiling, and tracing — part of observability (not just monitoring).

3. Causes (Bottom)
   The root cause of the issue. What exactly failed and why?
    Examples: A slow DB query, A memory leak in version 1.2.5, One microservice timed out, A deployment broke a config
This part requires active investigation using observability signals (those pillars).

Reactive vs Proactive:
 Traditional monitoring is reactive → "Alert fired, now go check"
 Observability enables proactive debugging → "Let's find the root cause using data"

  DevOps Example to Make It Real:
   You get an alert: “API latency > 3s”
   Monitoring tells you: There’s a spike
   Observability tells you:
   Logs: A timeout occurred in payment service, 
   Metrics: CPU on one pod is 100%,
   Traces: Slow DB query took 2.8s,
   Cause: DB index missing → fix.
  Without observability, you’re guessing. With observability, you’re investigating with data.
   observability is essential when systems get complex and distributed.

 What is Instrumentation?
  Instrumentation means adding monitoring code in the application so you can track internal behavior, especially when errors are not easily visible.
  In Java, developers often use:
    System.out.println("Step X reached. Variable value: " + myVar);
   This log gets printed when a specific block is executed.
  This helps understand what the system was doing before it failed.
  Developers add such lines intentionally, even if no error occurs, just to trace and measure.

  What is Wisdom of Production?
    If something breaks in production, use that experience to improve the code.
    The DevOps or Observability team sees the issue in production (like a latency spike), informs the development team, and they update the code by instrumenting it better.
So, next time, instead of guessing, logs and metrics will reveal what went wrong.

What is OpenTelemetry?
  OpenTelemetry is a tool that helps you watch what your app is doing.
  It helps collect:

Traces – what happened and when

Metrics – how much or how long something took

Logs – messages about what your app is doing

eg: If a website is slow, OpenTelemetry helps you:
  Track where the request went,
  See which service or database is slow,
  Understand the full path of the request.


1. OpenTelemetry Enables Auto-Instrumentation
     Manual instrumentation = Developers write extra code to track metrics, logs, traces.

     Auto instrumentation means OpenTelemetry can automatically track things like website requests and database calls without you writing extra code.

   This is critical because:

           Modern apps = 1000s of microservices, libraries, layers.
           
           You can’t manually trace everything — too slow and error-prone.
           
            OpenTelemetry makes observability scalable and developer-friendly.

 2.  Observability → Visibility Into Unknown Issues

Traditional monitoring finds known issues (e.g., CPU > 90%).

Observability helps you detect unknown problems — like:

       A new bug after deployment
       
       A service timing out randomly
       
       A hidden slow API dependency

3. Incident Resolution Becomes Faster and Smarter
   Observability makes it easier to:
     Detect an issue early (fast alerts)
     Debug it quickly using traces, logs, metrics
     Fix it faster → Reduce downtime
   
4. Reducing Operational Cost (Toil Reduction)
   Toil reduction means removing or reducing boring, repetitive, manual tasks that don't add much value — especially when they can be automated.
   Support work costs money. To save money:

         Use chatbots to fix common issues (25% incidents solved = big savings).
         
         Automate boring, repeated tasks (like restarting a service).
         
         In SRE, this boring work is called toil — it’s manual, repetitive, and can be automated. So, we try to remove it.

5.  Measure Team Velocity and Business Value:
It’s not just about releasing features — it’s about delivering real value.

So, we observe things like:
     
     How much work the team completes (e.g., story points)
     
     How often we release (deployment frequency)
     
     How fast users benefit from our changes

6. SLA → SLO → SLI
   "Move from Service Level Agreements to Service Level Objectives."
   
| Term    | What it Means                                                                    |
| ------- | -------------------------------------------------------------------------------- |
| **SLA** | External, legal: "We promise 99.9% uptime"                                       |
| **SLO** | Internal, engineering goal: "Let’s keep latency under 300ms for 95% of requests" |
| **SLI** | Actual measurement: “Latency p95 = 278ms”                                        |

SLOs help you measure and improve user experience realistically, not just legally.


KEY BENEFITS OF OBSERVABILITY:
<img width="1375" height="844" alt="image" src="https://github.com/user-attachments/assets/ba647408-8baa-49e9-8675-91fd70917501" />

GOOGLES GOLDEN SIGNALS OF MONITORING:
1. traffic:  How much demand your system is handling eg: requests
2. errors:  How often your system fails
3. latency:  How long your system takes to respond
4. saturation: How full or overworked your system is. eg: CPU at 100%, database connections maxed out

Tools used to monitor these signals:
      Logs, Events, Metrics, Tracing 

business metrix:
  number=tells how well ur company is going
  eg: customer churn(how many customers leave)
      revenue

Distributed Tracing:
   Distributed Tracing helps you track a request as it moves through multiple microservices, so you can find where delays or errors happen in complex systems.
   Shows the full journey of a request (e.g., frontend → auth service → payment service → DB)
   Imagine one request takes 3 seconds:

          0.2s in frontend
          
          0.5s in auth
          
          2.3s in payment → 🛑 here’s the bottleneck!
          
          Distributed tracing helps visualize and debug these chains.
          
 RED and USE Metrics Frameworks: Monitoring Strategies
     RED: Requests, Errors, Duration
     Used mainly for web services (e.g., REST APIs)
     
| Metric   | Meaning                           |
| -------- | --------------------------------- |
| Requests | Number of requests per second     |
| Errors   | Rate of failed requests           |
| Duration | Time taken to handle each request |

Helps you track how your service is behaving from a user’s view.

   USE = Utilization, Saturation, Errors:
   Used for infrastructure (e.g., servers, VMs, containers)

| Metric      | Meaning                                               |
| ----------- | ----------------------------------------------------- |
| Utilization | % of time a resource is busy (e.g., CPU usage)        |
| Saturation  | How much demand exceeds capacity (e.g., queue length) |
| Errors      | Count of failures (e.g., disk read/write errors)      |

Helps you monitor resource health — CPU, disk, memory, network, etc.
   
  
OBSERVABILITY MODEL:
<img width="1442" height="762" alt="image" src="https://github.com/user-attachments/assets/862c2711-9806-4331-8417-dfdd2e39141a" />

1. monitoring: traffic,latency,thoughput
2. Observability: Involves Metrics, Events, Logs, and Traces (often called MELT).Helps you understand why something went wrong.
3. Causal Observability: Goes deeper into finding the root cause:
       Looks at topology (how services are connected).

      Checks the exact time something failed.
      
      Uses historical data to investigate the issue.
4. Proactive Observability: Goes beyond just reacting to problems.
   Uses AI/ML to:

        Predict issues before they happen.
        
        Improve system reliability and automation.


SERVICE MAP:
	shows connections b/w services.
 	service map displays those connections graphically.
  	displaying relationships graphically.
  
TOPOLOGY BASED OBSERVABILITY:
	Topology shows how everything in your system is connected, and helps you spot issues quickly — like a GPS for your services.
 	Red colour=problem
  	green colour=good
   It's like having a map of your system that shows who is talking to whom, and using that map to find out where problems are happening and why.
   eg:
   Imagine your system has:

		Frontend → API → Auth service → Database
		If the frontend is slow, topology-based observability helps you see the entire path and quickly detect which connection is causing the delay.

TIME TRAVEL OBSERVABILITY:
It means being able to go back in time and see what happened in your system when something went wrong — like rewinding a video to find the exact moment an issue started.

Why is it needed?
<img width="811" height="346" alt="image" src="https://github.com/user-attachments/assets/c941d17f-9f52-4393-9d96-84cc75be890d" />

<img width="881" height="474" alt="image" src="https://github.com/user-attachments/assets/2372538c-2268-49e5-9574-acc5a4c2ddee" />


4 T'S:
1. Topology
2. Telemetry
3. Tracing
4. Time
   <img width="1364" height="502" alt="image" src="https://github.com/user-attachments/assets/eb139d17-38f2-42f9-9f3d-2610c90987d1" />

   when updating the microservice suddenly a latency spike happnes y? due to bugs, traffic, or dependencies,More pods or servers are started (auto-scaling) to handle the load.New Problem(Now the database is struggling. It starts timing out.),The service becomes fully unavailable .

DATAOPS:

Q. Why observability needs dataOps?
Ans: <img width="1571" height="676" alt="image" src="https://github.com/user-attachments/assets/b7a2094b-1d96-402b-b5e5-b010f6735157" />
	Data Agility:
		Systems keep changing — new apps, updates, errors.
	 	DataOps helps observability adapt quickly to these changes.
   	Data Flow Management:
    		 Tons of data (logs, metrics, traces) are coming in.
       		 DataOps organizes and controls how this data moves — so nothing important is missed.
	Automation:
 		You can’t handle observability manually — too much data.
   		DataOps automates how data is collected, processed, and sent to tools (like Grafana, Splunk, etc.)
     	Data Security:
      		Observability data might contain sensitive info.
		DataOps keeps the data safe — by controlling access, masking, encrypting, etc.

	Extracting Value from Data:
 		Raw data is messy and hard to understand.
		DataOps cleans and prepares the data, so we can find issues faster and make better decisions.
TRAID:
<img width="1445" height="705" alt="image" src="https://github.com/user-attachments/assets/353b1f73-bff9-4c36-9129-0abb7aa3ce18" />


CIA TRAID:
1. confidentiality: Ensures only authorized people can access the data
2. integrity: Ensures data is accurate and not tampered with. (e.g., checksums, hashing)
3. availability: Ensures systems/data are available when needed. (e.g., backups, load balancing)

DEI TRAID:
1. distributed: Systems are spread across many servers or locations (e.g., microservices in different regions)
2. Ephimeral: Components don’t last long — they can be created and destroyed quickly. (e.g., containers)
3. immutable: Once created, they don’t change. Instead of editing, new versions are deployed. (e.g., immutable infrastructure)

OBSERVABILITY PIPELINE:
An observability pipeline is the system or flow that collects, processes, and routes observability data (like logs, metrics, and traces) from your applications and infrastructure to tools like Grafana, Datadog, or Azure Monitor.
collects data and sents to monitoring tools.

<img width="1417" height="712" alt="image" src="https://github.com/user-attachments/assets/53d2a82c-bf74-4c5a-9d53-ef9c63e9d485" />

1.Transmission:
	 (How data moves through the pipeline)
  	The movement of observability data (logs, metrics, traces) from source to destination.
  	 Push-based: The agent sends data to the backend (e.g., Prometheus Pushgateway).

	Pull-based: The backend pulls data from the source (e.g., Prometheus scraping metrics).
 
 2.  Protocols (Rules for communication between tools)
    
| Type          | Examples                              | Used For            |
| ------------- | ------------------------------------- | ------------------- |
| **Logs**      | Syslog, Fluent Protocol, HTTP         | Log forwarding      |
| **Metrics**   | Prometheus, OpenMetrics, StatsD       | Time-series data    |
| **Traces**    | OTLP (OpenTelemetry Protocol), Jaeger | Distributed tracing |
| **Transport** | HTTP, gRPC, Kafka, MQTT               | Sending data        |

3. Storage (Where observability data is stored)
   	Once collected and processed, data must be stored for querying, alerting, and analysis.
   
| Data Type | Storage Tools                       | Notes                       |
| --------- | ----------------------------------- | --------------------------- |
| Logs      | Elasticsearch, Loki, Splunk         | Searchable, compressed      |
| Metrics   | Prometheus, InfluxDB, Azure Monitor | Stored as time series       |
| Traces    | Jaeger, Tempo, Zipkin               | Stored in trace DBs or blob |

Q. What is the ELK Stack?
<img width="1332" height="804" alt="image" src="https://github.com/user-attachments/assets/63aef745-bc8a-4b16-817b-a72198e39541" />
ELK = Elasticsearch + Logstash + Kibana
Elasticsearch=storage
Logstash=Processor
Kibana=UI


Logstash – Collects and processes logs from different sources.

Elasticsearch – Stores and searches the logs efficiently.

Kibana – Shows the logs visually on dashboards and graphs.


Q. shift from monitoring to observability?

	1. Do metrix, traces, logs
 
 	2. Opentelemetry: An emerging open standard (by Cloud Native Computing Foundation) that automatically collects data from your systems (auto-instrumentation).
  
  	3. AI & Machine Learning: Used to analyze and learn from the collected data for smarter decisions.

   	4. Data Aggregation: Combine all data into a single source of truth so it’s easier to understand and work with

    	5.  Data Analysis:
     		Use slicing and dicing to extract useful insights from that data.

	 Auto Remediation:
		Fix issues automatically so you save time and resolve problems faster.

  OPENTELEMETRY:
  <img width="1523" height="695" alt="image" src="https://github.com/user-attachments/assets/5b3941c2-956d-428f-945d-1aeca74acc7b" />

  Challenges with the current tools with openTelemetry?

  1. OpenTelemetry:
      It's a vendor-neutral open-source project.
      Helps collect logs, metrics, and traces.
      Supported by the Cloud Native Computing Foundation (CNCF).

 2. Why OpenTelemetry?
    	Replaces multiple agents/tools with one unified standard.
    	Reduces vendor lock-in
    	Makes auto-instrumentation easier for microservices.

CHALLENGES OF OBSERVABILITY
 1. Environment Inconsistency: ur code works in 1 env(dev) not in other(prod). It makes debugging difficult because bugs don’t happen everywhere.
    
 2. Too Many Traces: The system collects a lot of tracing data.It's hard to find useful information, and storage/performance is affected.
    
 3.  Unnecessary Alerts: You get too many alerts, even for minor or non-critical issues.Teams start ignoring alerts, and might miss real problems.
    
 4.  Losing Focus on Customer Experience: Dashboards show everything is fine, but users still face problems.Teams look only at system health, not at user issues.
    
 4.  Too Many Dashboards: There are many dashboards showing different data.Teams get confused, don’t know where to look for answers.
    
 5. Can’t Trace the Full Request (End-to-End):  Logs or traces don’t show the complete journey of a user request.It’s hard to find out exactly where things went wrong.

    OBSERVABILITY TOOL:
    1. user friendly  interface (ease of navigation and commercial value)
    2. supplies real time data (API's to collect real data)
    3. works on open source agents(reduce system memory & cpu, security and configuration.
    4. Easy to implement
    5. Integrates with current tools(current stacks,languages,container platform,messaging)






     

