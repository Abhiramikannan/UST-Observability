1. SPLUNK:
      like a search engine for all the machine generated data from ur computer,apps,devices..
      collects all the messy,unstructured data like login failed,user login and organizes it into a searchable format.
      it will make a label for similar data -> easier for search
      This process makes it easy to find and analyze specific events across all your systems, even if they're from different types of devices or applications.
      
2. What is a Splunk Instance?   
          A Splunk instance is a single installation of the Splunk software. 
          Think of it as a single copy of the program running on a computer.
          In a small setup, one instance can handle everything: taking in the data, organizing it, and letting you search it.
          In larger companies, they often use many Splunk instances that work together.
          Some are dedicated to collecting and organizing data (these are called indexers), and others are for running searches and showing dashboards (these are called search heads).
          This setup allows Splunk to handle massive amounts of data and searches at the same time without slowing down.
          
3. What does "Ingesting the Data" mean?
            Ingesting the data simply means getting the data into Splunk. It's the first step in the entire process
            Imagine Splunk as a giant storage locker for all your digital notes and logs. 
            Ingesting is the process of physically moving those notes from where they were created (like a web server or an app) and putting them inside that storage locker.
            Splunk can ingest data in different ways:
                        forwarders: like a delivery trucks put on all ur computers where they collect logs files as they are created and sent them to main splunk instance
                        API and NETWORK PORTS: 
                              NETWORK PORT : Its like a numbered door (eg: port 80)
                                             each computer have many network ports where the applications are listening to
                                             Imagine ur splunk instance is like a house
                                             To get the data inside the house(splunk instance), you need to open the door
                                             Each network port is like a different numbered door on the house
                              Splunk Listens at the Door:
                                             You can tell Splunk to stand at a specific door and wait for data to arrive. This is called "listening" on a port
                              HEC (HTTP Event Collector) is a Special Mailbox:
                                              Special Mailbox (HEC): HEC is like a special, secure mailbox you put outside your house (Splunk).
                                              Applications can directly drop their data into this mailbox using a specific key (a "token").
                                              This is a very clean and direct way to get data in, especially from modern web applications.
                                              the data from different application talk to eachother using API so using spluck you can create a input acalled hec and it can sent the data to the port where splunk is listening
                              Network Ports (TCP/UDP):
                                            some data can be tranferred through port not api
                                            sent log data over internet through certain ports
                                            the data  comes through the port of tcp and udp can be fetched..
                                            by setting up the port the splunk can listen on that port and grab the information
                              
                                                             

4. Can Splunk get data from all environments?
      yes. 
      from cloud(aws,google),computers
      It uses lightweight agents called forwarders to collect data from remote machines and forward it to a central Splunk instance.

5. SPLUNK USES A DATABASE-SPL?
           Splunk works with unstructured data (like logs), not the neat, organized tables of a traditional database. 
           You don't use a standard database query language like SQL. 
           Instead, you use Splunk's own flexible language, SPL (Search Processing Language), to find patterns and relationships within the data.
           This makes it a powerful search tool, not a traditional database.
           
6. Dashboard and panels in splunk?
              can save each specified search as 1 panel
              eg: ou run one specific search (for example, to count the number of failed logins), and that single count will be displayed in one panel on your dashboard.
              Each panel is dedicated to showing the result of its own specific search.

7. Transforming commands?
                  Transforming commands in Splunk change your raw data into a summarized view, like turning a long list of events into a simple chart or table.

8. BOOLEAN OPERATIONS IN SPLUNK?
            AND
            OR
            NOT
            ()
            ""
            \""
      AND is used to find events that contain all of the specified terms. This narrows your search. For example, login AND failed will only show events that have both the word "login" and the word "failed."
      OR is used to find events that contain at least one of the specified terms. This broadens your search. For example, error OR warning will show you any event that contains either "error" or "warning" (or both).
      NOT is used to exclude a specific term from your search results. For example, failed NOT password will show all events with the word "failed" but will remove any that also have the word "password." You can also use a minus sign         (-) as a shorthand for NOT, such as -password
      You can also use parentheses () to group terms and control how the Boolean logic is applied. For example, (login AND failed) OR error will find events that are either "login failed" or just the word "error."

9. Splunk search querry?
            index=network sourcetype=cisco_wsa_squid usage=Violation 
            | stats count(usage) as Visits by cs_username 
            | search Visits > 1

      | -> passes the outcome to the next
      stats count(usage) as Visits: This is a transforming command. It tells Splunk to count the number of times usage appears. It renames the count to a new field called Visits.
      search Visits > 1: This tells Splunk to show only the results where the new field Visits is greater than 1.

10. KNOWLEDGE OBJECTS?
             knowledge objects are things you create and save so Splunk can use them to better understand your data. They are the rules and definitions you build to make your data smarter.
             eg: When you create a search command and save it to use again later, that is a type of knowledge object called a Saved Search or Report.
                  It's a way to save your work so you don't have to rebuild the search every time, making it much easier and faster for you and others to get the same results.
            lookup is a knowledge object -> Adds extra information to your data based on a separate file you provided.
            Report is a knowledge object-> Re-runs the same search every time.

11. 3 ROLES ?
            ADMIN
            POWER
            USER

12. KNOWLEDGE OBJECTS ARE GROUPED INTO 5 CATEGORIES?
            Data Classification, -> classifying data into diff categories(tags to group similar events)
            Data Normalization,-> This is the process of cleaning and standardizing your data. eg: making sure all the dates are in the same format 
            Data Enrichment, and -> This step adds new, valuable information to your existing data. For instance, if your data has an IP address, you can "enrich" it by adding the city and country that IP address is from.
                                    This is often done using a lookup table
            Data Interpretation -> This is the final step where you look at the cleaned, enriched data to find meaningful insights, like trends or patterns. This is where you might create charts or reports to visualize your findings.
                                    Data Interpretation is done by creating Reports and Dashboards which use your classified, normalized, and enriched data to find insights.

13. can we add the report as panels in the dashboard in splunk?
            yes
            When you save a search as a report, it becomes a knowledge object that you can reuse. 
            One of the most common ways to reuse a report is to add it as a panel on a dashboard. 
            This allows you to display the results of that report alongside other information, providing a comprehensive, high-level view of your data in a single place.

14. can also edit the drill down in each panel..why it is used?
            detailed view of information

15. what is dashboard studio?
            Dashboard Studio is a modern dashboard-building environment in Splunk.
            easy to customize ur dashboard.
            
16. By default, who is able to view a saved report?

                  Any user with the viewreports capability
                  
                  The user who created it -> CORRECT
                  
                  The user who created it or any user with an admin role
                  
                  Any user with a power or admin role
17. What is the most efficient way to limit search results returned?

                  host
                  
                  time
                  
                  indeX
                  
                  source

18. search will active upto  10 mins

19. DEFAULT ROLE OF SPLUNK ENTERPRICES:
            USER
            POWER
            ADMIN

20. Which search mode behaves differently depending on the type of search being run?

                  Variable
                  
                  Smart 
                  
                  Verbose
                  
                  Fast
21. By default, which of the following roles are required to share knowledge objects?

                  Admin
                  
                  Power
                  
                  User
                  
                  Manager

22. Which of the following searches will return results containing the terms failed, password, or failed password?

                  failed OR password OR "failed password"
                  
                  failed OR password -> CORRECT
                  
                  failed password OR "failed password"
                  
                  fail*

23. Which Splunk infrastructure component stores ingested data?

      Index 
      
      Data models
      
      Dashboards
      
      Datasets

24. Which command can be used to further filter results in a search?

      search 
      
      subset
      
      subsearch
      
      filter

25. When a search is run, in what order are events returned?

            Reverse chronological order 
            
            Alphanumeric order
            
            Chronological order
            
            Reverse alphanumeric order

26. Which character is used in a search before a command?

            A quotation mark (")
            
            A backtick (`)
            
            A tilde (~)
            
            A pipe (|) 

27. What determines the timestamp shown on returned events in a search?

            Timestamps are displayed in epoch time
            
            The time zone where the event originated
            
            The time zone defined in user settings 
            
            Timestamps are displayed in Greenwich Mean Time



1/9/25
-------------------------------------------
USING FIELDS:
-------------------------------------
sourcetype=linux_search  -> returns only that events.(values)
field names are case sensitive,values are not

FIELD OPERATORS:
      =
      != 
      These are used in numerical and strings

      >
      <
      >=
      <= 
      these are used with numericals.

fields can added into search by clicking on the value in the fields window
can also use boolean in field value pair

eg:
index=security sourcetype=linux_secure action=failure NOT (host="mail*" OR host=www1)

index=security sourcetype=linux_secure action=failure: This part finds all events from the security index that are Linux security logs and have a "failure" action.

NOT (host="mail*" OR host=www1): This is a Boolean filter. It excludes any of the previous results where the host name either starts with "mail" (the asterisk is a wildcard) or is exactly "www1."

------------------------------------
The Difference between != and NOT:
----------------------------------------------------
            != status should be there and check condition
            Not: if status not there also it will return all the events after excluding status=200

      Imagine you have three events:

      status=200 user=john
      status=404 user=jane
      user=mary (no status field)

Search for status!=200: This would return only event #2 (status=404) because event #1 is a match and event #3 doesn't even have a status field to compare.

Search for NOT status=200: This would return events #2 and #3. It excludes event #1 because it has status=200, but includes event #3 because its status field is not 200 (it's null or non-existent).
---------------------------------------------
OR:
--------------------------------------------
             index=web (status=500 OR status=503 OR status=505)
                  tells splunk to look only in the index=web first
                  then go to paranthesis to group the condition -> status is either 500 or 503 or 505

-----------------------------------------------
IN:
----------------------------------------------
      index=web status IN ("500", "503", "505")
            shortcut for a series of OR operator

-----------------------------------------------------

PLACE FIELDS COMMAND BEFORE STATS:
----------------------------------------
index=web status IN ("500", "503", "505")
| fields status
| stats count by status
      
      placing the fields command before the stats command is a best practice for search performance.
      The fields command is a search filter. It tells Splunk to keep only the fields you specify.
      The stats command is a powerful transforming command in Splunk. It's used to calculate statistics on your search results, like counts, averages, sums, and totals.


also can give + to include the field and - to exclude the field 

      index=web status IN ("500", "503", "505")
      | fields +status
      | stats count by status

------------------------------------------------
RENAME:
      index=web status IN ("500", "503", "505")
      | fields status
      | stats count by status
      | rename status as "HTTP Status", count as "Number of Events"

            it changes the column name in the final tables.
            status will be HTTP Status and Count will be Number of events.
-------------------------------------------------------------------------

FIELDS IN SEARCH RESULTS:
      When Splunk ingests data, it automatically extracts a few key pieces of information from each event. These pieces are called default fields.
      These fields always include the source, sourcetype, and host. Splunk also identifies and extracts the timestamp of the event, which it stores as the _time field.
      This process happens automatically as soon as the data comes into Splunk, so you can immediately begin searching and filtering using these fields.

      host
      source
      sourcetype
      time
      raw

event: text ..like logs (user logged,A file being saved.)
DEFAULT SELECT FIELDS:
      HOST
      SOURCE
      SOURCETYPE

            --------------------------------------------------
      1. EVAL:
            The eval command is a powerful command that creates a new field in your search results by evaluating an expression. You can use eval to perform calculations, convert data types, or manipulate field values.

index=network sourcetype=cisco_wsa_squid
| stats sum(sc_bytes) as Bytes by usage
| eval bandwidth = Bytes/1024/1024

This search first calculates the total bytes for each type of network usage. The eval command then converts those total bytes into a more readable format (megabytes) and stores it in a new field called bandwidth

-------------------------------------------------------------
FIELD EXTRACTION:
      used to extract fields from ur data
-------------------------------------------------------
COMMAND EXTRACTION:
      |rex
      |erex

1. erex: The erex command is used to simplify the process of field extraction. It automatically generates a regular expression for you based on examples you provide.
-----------
index=games sourcetype=SimCubeBeta | erex Character fromfield=_raw examples="pixie, Kooby"
            This search looks in the games index for data from SimCubeBeta. The erex command then tells Splunk to create a new field named Character by looking at the raw data (_raw) and using "pixie" and "Kooby" as examples of what to extract.


index=games sourcetype=SimCubeBeta | erex Character fromfield=_raw examples="pixie, Kooby" | where isnull(Character)
                  This query first extracts a new field called Character using erex. The where command then filters the results to show only the events where the Character field is null (meaning, it couldn't find a character to extract). This is a useful way to find and fix events that didn't follow the pattern you expected.

click the job in the side to see REGULAR EXPRESSION.


2. rex:
--------------
      index=games sourcetype=SimCubeBeta | rex field=_raw "^\S*\s*\h(?<User>[a-zA-Z0-9_-]+)@(?<Character>[a-zA-Z0-9-]+)"
            This search query is used to find and extract two new fields—User and Character—from your raw event data.
            | rex field=_raw "...": This is the command that applies a regular expression to the raw event data (_raw). The complex pattern in quotes is what tells Splunk exactly what to find and extract.
            (?<User>[a-zA-Z0-9_-]+): This is the first part of the pattern. It tells Splunk to look for a username and save it into a new field called User.
            @: This is a literal character that acts as a separator.
            (?<Character>[a-zA-Z0-9-]+): This is the second part of the pattern, which finds the character name and saves it into a new field called Character


The Difference between rex and erex:
-------------------------------------------
rex (Regular Expression): This is the manual way to extract fields. You have to write the entire regular expression yourself. This gives you very precise control but requires knowledge of regular expressions. It's used when you know the exact pattern you need to find.

erex (Extract Regular Expression): This is the automatic way. You don't have to write a regular expression. Instead, you provide a few examples of the data you want to extract, and Splunk automatically figures out the pattern for you. It's much easier for users who aren't familiar with regular expressions.
-------------------------------------------

ENRICHING DATA WITH KNOWLEDGE OBJECTS:
-----------------------------------------
Enriching data with knowledge objects means using reusable, saved settings in Splunk to add new, valuable information to your events.The most common way to do this is with a lookup table. For example, if your logs only show an IP address like 10.1.2.3, you can use a lookup to automatically add the server's name (Sales_Server) and its location (Headquarters) to every event with that IP.

This process adds context and makes your data much more useful for searching, reporting, and analysis.
-------------------------------------------------------------



VISUALIZATION:
------------------------------------------------
fields - fieldname fieldname : all fields after - should be affected if we give - and space after that
fields -fieldname fieldname : only the field behind the - will get affected.

|table
--------------
index=web sourcetype=access_combined product_name=*
| table JSESSIONID product_name price

This search finds all web access events that include a product_name. It then displays a table showing only the JSESSIONID, product_name, and price fields for each event. All other fields are dropped.
The table command is a transforming command used to create a simple table from your search results. It's a key part of the search process because it lets you quickly see only the fields you care about.

adding fields to limit the search:
      index=web sourcetype=access_combined product_name=* | fields JSESSIONID price product_name
      | table JSESSIONID price product_name

      The | fields command then tells Splunk to keep only the JSESSIONID, price, and product_name fields. All other fields in the events are dropped at this point.

----------------------------------
|dedup command:

      index=web sourcetype=access_combined product_name=*
      | fields JSESSIONID price product_name
      | table JSESSIONID price product_name
       | dedup JSESSIONID price

Remove duplicated values
The dedup command is used to remove duplicate events from your search results. When you use dedup, Splunk keeps the first event that it finds with a unique combination of the fields you specify and discards all the rest. It's a useful way to clean up your search results and get a list of unique values.

----------------------------------------------
|addtotals
======================================
index=sales sourcetype=vendor_sales product_name=* VendorCountry="United States" OR VendorCountry="Canada"
| chart sum(price) over product_name by VendorCountry
| addtotals col=true


addtotals → Adds a row total (sums down the columns).
addtotals col=true → Adds a column total (sums across the row).

without col=true example:
| product\_name | United States | Canada |
| ------------- | ------------- | ------ |
| Laptop        | 5000          | 3000   |
| Mobile        | 7000          | 2000   |
| **Total**     | 12000         | 5000   |

with col=true:
| product\_name | United States | Canada | **Total** |
| ------------- | ------------- | ------ | --------- |
| Laptop        | 5000          | 3000   | 8000      |
| Mobile        | 7000          | 2000   | 9000      |


with label:
======================
index=sales sourcetype=vendor_sales product_name=* VendorCountry="United States" OR VendorCountry="Canada"
| chart sum(price) over product_name by VendorCountry
| addtotals col=true label="Total Sales" labelfield="product_name"

without label field:
            | addtotals col=true label="Total Sales"
      The extra column appears, but in the product_name column, the cell for that totals row is blank.

| product\_name | United States | Canada | Total Sales |
| ------------- | ------------- | ------ | ----------- |
| Laptop        | 5000          | 3000   | 8000        |
| Mobile        | 7000          | 2000   | 9000        |
| *(blank)*     | 12000         | 5000   | 17000       |

With labelfield="product_name":
            | addtotals col=true label="Total Sales" labelfield="product_name"
Now the totals row is not blank in product_name. Instead, it shows the word “Total Sales”.

| product\_name   | United States | Canada | Total Sales |
| --------------- | ------------- | ------ | ----------- |
| Laptop          | 5000          | 3000   | 8000        |
| Mobile          | 7000          | 2000   | 9000        |
| **Total Sales** | 12000         | 5000   | 17000       |

Without labelfield → that row is blank.
With labelfield="product_name" → it displays the text you gave in label.


=====================================================
index=sales sourcetype=vendor_sales product_name=* VendorCountry="United States" OR VendorCountry="Canada"
| chart sum(price) over product_name by VendorCountry
| addtotals col=true label="Total Sales" labelfield="product_name" fieldname="Total By Product"


fieldname="Total By Product" → Renames the totals column itself from the default “Total” (or “Total Sales” if you used label) into “Total By Product”.

| product\_name   | United States | Canada | **Total By Product** |
| --------------- | ------------- | ------ | -------------------- |
| Laptop          | 5000          | 3000   | 8000                 |
| Mobile          | 7000          | 2000   | 9000                 |
| **Total Sales** | 12000         | 5000   | 17000                |
=============================================================================

index=sales sourcetype=vendor_sales product_name=* VendorCountry="United States" OR VendorCountry="Canada"
| chart sum(price) over product_name by VendorCountry
| addtotals col=true label="Total Sales" labelfield="product_name" fieldname="Total By Product" row=false

row=false → Prevents adding a totals row at the bottom.

| product\_name | United States | Canada | Total By Product |
| ------------- | ------------- | ------ | ---------------- |
| Laptop        | 5000          | 3000   | 8000             |
| Mobile        | 7000          | 2000   | 9000             |

There is NO extra row at the bottom (because of row=false).

============================================================================
|fieldformat
===============================================
index=sales sourcetype=vendor_sales product_name=*
| stats sum(price) as Total by product_name
| addtotals col=t label="Total Sales" labelfield="product_name"
| fieldformat Total = "$" + tostring(Total, "commas")

fieldformat Total = "$" + tostring(Total, "commas")
Formats the Total field.
Adds $ sign in front.
Converts numbers into comma-separated format (e.g., 12,000 instead of 12000).

| product\_name   | Total    |
| --------------- | -------- |
| Laptop          | \$5,000  |
| Mobile          | \$7,000  |
| **Total Sales** | \$12,000 |


SPLUNK TRANSFORMMING COMMANDS:
==================================
top
rare
stats
chart
timechart
trendline


1. top:

index=sales sourcetype=vendor_sales
| top Vendor


The top command finds the most common values in a field.
Here, it looks at the Vendor field.
By default, it returns the top 10 most frequent Vendors along with:

count → how many times each Vendor appeared
percent → percentage of total events


Example:

Suppose your sales data has these vendors:
Vendor A (100 sales)
Vendor B (70 sales)
Vendor C (30 sales)
The top Vendor output will look like:

| Vendor   | count | percent |
| -------- | ----- | ------- |
| Vendor A | 100   | 50%     |
| Vendor B | 70    | 35%     |
| Vendor C | 30    | 15%     |


top Vendor helps you quickly find the most popular vendors in the dataset.
You can also customize it, e.g., top 5 Vendor (only top 5 vendors) or top Vendor limit=3 (top 3 vendors).
=========================================
index=sales sourcetype=vendor_sales
| top Vendor limit=20

limits the vendors upto 20
=========================================


TOP COMMAND CLAUSE
=======================
countfield = string   ->COUNT COLUMN WILL BE RENAMED AS  STRING
percentfield = string
showcount = true/false
      | top Vendor showcount=false
                  Will not display the count column.

showperc = true/false
      | top Vendor showperc=false
            Will not display the percent column.

showother = true/false

================================================

CLAUSE:
      1. BY:
            index=sales sourcetype=vendor_sales
            | top product_name by Vendor limit=3 countfield="Number of Sales" showperc=false

Search in the sales index with data coming from vendor_sales sourcetype.
For each Vendor, find the most common product_name values.
Show only the top 3 products per Vendor.
Rename the count column (normally just count) to Number of Sales.
Don’t show the percentage column.

====================================
|rare
=======================
The rare command in Splunk is a powerful tool used to identify and display the least common values in a dataset.
============================


COMMON STAT FUNCTIONS
===============================
sum
count
distinct count
average
min
max
value
list
====================================

usenull=f  -> to remove the null
useother=f ->to remove other column
=================================

Trendchart:
      sma(Simple Moving Average – equal weights)
      ema (Exponential Moving Average – higher weight to recent)
      wma (Weighted Moving Average – manual weights)


|iplocation command:
==========================
add location information from a 3rd party database to an event

QUIZ
===================
Which of the following commands can return a count of all events matching search criteria over a specified time period? 
where
stats ->correct
match
count

Which command changes the appearance of field values?

rename
fieldformat ->correct
fields
format


In a single series data table, which column provides the x-axis values for a visualization?

The third column
The fourth column
The second column
The first column -> correct

Which clause can be used with the rare command to specify whether or not a percentage column is created?

showperc ->correct
displayperc
perccol
percentage

Which argument can be used with the geostats command to control the column count?

collimit -> correct
latfield
globallimit
longfield


Which command can be used to exclude fields from search results?

dedup
exclude
fields -> correct
remove


How can the order of columns in a table be changed?

By changing the order of fields specified in the fields command
By dragging and dropping in the table interface
By selecting the "Move column" option in a column header's dropdown
By changing the order of fields specified in the table command

Which type of default map visualization uses shading to represent relative metrics?
✅ Choropleth Map

When using the timechart command, which axis represents time?
✅ X-axis

How many columns are displayed in a visualization by default when using the chart command?
✅ 10
-----------------------------------------------

2/9/25
----------------------------------------
_time field: will take the timestamp



### 🔍 Searching by Time in Splunk:
---------------------------------------------

            * Every log or event in Splunk has a **time stamp** (the time when it happened).
            * When you search, you usually don’t want *all* data, only from a certain time.
            * Example:
            
              * "Last 15 minutes" → shows only events that happened in the last 15 minutes.
              * "Yesterday" → shows only yesterday’s events.
              * "Between Aug 25 and Aug 28" → shows only events in that range.

👉 In Splunk, you can set this in the **time picker** (top-right of the search bar).

eg: When you search in Splunk, you usually want results from a specific time range.

      If you want to see today’s logs:
                  index=web_logs earliest=@d latest=now
      @d means start of the day.

      If you want last 15 minutes logs: index=web_logs earliest=-15m
                              -15m means go back 15 minutes from now.
      If you want yesterday’s logs:
                  index=web_logs earliest=-1d@d latest=@d

---

### ⏰ Time Formatting in Splunk
-----------------------------------------

* Time in Splunk is stored in a standard format (like `2025-09-01 14:30:00`).
* But when showing or searching, Splunk can **format** it differently.
* Example formats:

  * `MM/DD/YYYY` → 09/01/2025
  * `DD-MM-YYYY` → 01-09-2025
  * `HH:MM:SS` → 14:30:00

👉 You can use commands like `eval` and functions (e.g., `strftime`) to **change how the time looks** in your results.

eg: Convert to MM/DD/YYYY format:
      ... | eval new_time=strftime(_time, "%m/%d/%Y")
                        If _time = 2025-09-01 10:30:00,
                        then new_time = 09/01/2025.
---

✅ In short:

* **Searching by time** = filter events by time range.
* **Time formatting** = how you *display* or *convert* the time.

-----------------------------------------------------------

